{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "suhwHjlVYTJX",
        "beTPg_llYhsl"
      ],
      "toc_visible": true,
      "mount_file_id": "169394ighnOWJXwhLcnlrwFdDIvgSpVs2",
      "authorship_tag": "ABX9TyPDVFJ2FHWXtJEVML5uQ5Qv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhambadaya/amazon-reviews-sentiment-classification/blob/main/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SfhdBYFSQb-",
        "outputId": "b74c0d61-33fb-46d3-e854-cc6a75fe28d2"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jun 26 19:21:05 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laXcliV5TSE3",
        "outputId": "7d284228-c7c5-4c14-bf7c-5143d05191b1"
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/a2/5ccf0a418eb22e0a2ae9edc1e7f5456d0a4b8b49524572897564b4030a9b/tensorflow_gpu-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3MB)\n",
            "\u001b[K     |████████████████████████████████| 454.3MB 37kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.36.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.5.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.12.4)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.7.4.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.5.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.34.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.4.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.19.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (57.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (1.31.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow-gpu) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow-gpu) (4.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow-gpu) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (0.4.8)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3uIVeF4uVrW",
        "outputId": "b2c986fc-678a-4561-8035-dedb39438d89"
      },
      "source": [
        "!tar -zxvf /content/drive/MyDrive/amazon_review_full_csv.tar.gz -C /content/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "amazon_review_full_csv/\n",
            "amazon_review_full_csv/train.csv\n",
            "amazon_review_full_csv/test.csv\n",
            "amazon_review_full_csv/readme.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL_rF6dgjkJd",
        "outputId": "4376174c-99d6-429b-fe45-b82e9525df43"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qSyKwYDhAeP"
      },
      "source": [
        "data_path = '/content/amazon_review_full_csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-CD_hwN2uqo",
        "outputId": "5f3b3705-b22b-4d43-c801-398e609a46d2"
      },
      "source": [
        "f = open(\"/content/amazon_review_full_csv/readme.txt\", \"r\")\n",
        "print(f.read())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Amazon Review Full Score Dataset\n",
            "\n",
            "Version 3, Updated 09/09/2015\n",
            "\n",
            "ORIGIN\n",
            "\n",
            "The Amazon reviews dataset consists of reviews from amazon. The data span a period of 18 years, including ~35 million reviews up to March 2013. Reviews include product and user information, ratings, and a plaintext review. For more information, please refer to the following paper: J. McAuley and J. Leskovec. Hidden factors and hidden topics: understanding rating dimensions with review text. RecSys, 2013.\n",
            "\n",
            "The Amazon reviews full score dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the above dataset. It is used as a text classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\n",
            "\n",
            "\n",
            "DESCRIPTION\n",
            "\n",
            "The Amazon reviews full score dataset is constructed by randomly taking 600,000 training samples and 130,000 testing samples for each review score from 1 to 5. In total there are 3,000,000 trainig samples and 650,000 testing samples.\n",
            "\n",
            "The files train.csv and test.csv contain all the training samples as comma-sparated values. There are 3 columns in them, corresponding to class index (1 to 5), review title and review text. The review title and text are escaped using double quotes (\"), and any internal double quote is escaped by 2 double quotes (\"\"). New lines are escaped by a backslash followed with an \"n\" character, that is \"\\n\".\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85EShEmii4hP"
      },
      "source": [
        "load_amazon_reviews_sentiment_analysis_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nio6UsrimkC"
      },
      "source": [
        "def load_amazon_reviews_sentiment_analysis_dataset(data_path, seed=123):\n",
        "    \"\"\"Loads the amazon reviews sentiment analysis dataset.\n",
        "    # Arguments\n",
        "        data_path: string, path to the data directory.\n",
        "        seed: int, seed for randomizer.\n",
        "    # Returns\n",
        "        A tuple of training and validation data.\n",
        "        Number of training samples: 3000000\n",
        "        Number of test samples: 650000\n",
        "        Number of categories: 5\n",
        "    # References\n",
        "        Zhang et al., https://arxiv.org/abs/1509.01626\n",
        "        Download and uncompress archive from:\n",
        "                https://drive.google.com/open?id=0Bz8a_Dbh9QhbZVhsUnRWRDhETzA\n",
        "    \"\"\"\n",
        "    columns = (0, 1, 2)  # 0 - label, 1 - title, 2 - body.\n",
        "    train_data = _load_and_shuffle_data(\n",
        "            data_path, 'train.csv', columns, seed, header=None)\n",
        "\n",
        "    test_data_path = os.path.join(data_path, 'test.csv')\n",
        "    test_data = pd.read_csv(test_data_path, usecols=columns, header=None,nrows= 10000)\n",
        "\n",
        "    # Get train and test labels. Replace label value 5 with value 0.\n",
        "    train_labels = np.array(train_data.iloc[:, 0])\n",
        "    train_labels[train_labels == 5] = 0\n",
        "    test_labels = np.array(test_data.iloc[:, 0])\n",
        "    test_labels[test_labels == 5] = 0\n",
        "\n",
        "    # Get train and test texts.\n",
        "    train_texts = []\n",
        "    for index, row in train_data.iterrows():\n",
        "        train_texts.append(_get_amazon_review_text(row))\n",
        "    test_texts = []\n",
        "    for index, row in test_data.iterrows():\n",
        "        test_texts.append(_get_amazon_review_text(row))\n",
        "\n",
        "    return ((train_texts, train_labels), (test_texts, test_labels))\n",
        "\n",
        "\n",
        "#========================================================================\n",
        "def _get_amazon_review_text(row):\n",
        "    \"\"\"Gets the Amazon review text given row data.\n",
        "    # Arguments\n",
        "        row: pandas row data from Amazon review dataset.\n",
        "    # Returns:\n",
        "        string, text corresponding to the row.\n",
        "    \"\"\"\n",
        "    title = ''\n",
        "    if type(row[1]) == str:\n",
        "        title = row[1].replace('\\\\n', '\\n').replace('\\\\\"', '\"')\n",
        "    body = ''\n",
        "    if type(row[2]) == str:\n",
        "        body = row[2].replace('\\\\n', '\\n').replace('\\\\\"', '\"')\n",
        "    return title + ', ' + body\n",
        "\n",
        "#========================================================================\n",
        "def _load_and_shuffle_data(data_path,\n",
        "                           file_name,\n",
        "                           cols,\n",
        "                           seed,\n",
        "                           separator=',',\n",
        "                           header=0):\n",
        "    \"\"\"Loads and shuffles the dataset using pandas.\n",
        "    # Arguments\n",
        "        data_path: string, path to the data directory.\n",
        "        file_name: string, name of the data file.\n",
        "        cols: list, columns to load from the data file.\n",
        "        seed: int, seed for randomizer.\n",
        "        separator: string, separator to use for splitting data.\n",
        "        header: int, row to use as data header.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    data_path = os.path.join(data_path, file_name)\n",
        "    data = pd.read_csv(data_path, usecols=cols, sep=separator, header=header,nrows=2000000)\n",
        "    return data.reindex(np.random.permutation(data.index))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-_eRCKrXtOn"
      },
      "source": [
        "###loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Gd7g9dqyiynw"
      },
      "source": [
        "(train_texts, train_labels), (test_texts, test_labels) = load_amazon_reviews_sentiment_analysis_dataset(data_path,123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suhwHjlVYTJX"
      },
      "source": [
        "### preparing data for ngram model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv8DS1N9RQYL"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "# Vectorization parameters\n",
        "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
        "NGRAM_RANGE = (1, 2)\n",
        "\n",
        "# Limit on the number of features. We use the top 20K features.\n",
        "TOP_K = 20000\n",
        "\n",
        "# Whether text should be split into word or character n-grams.\n",
        "# One of 'word', 'char'.\n",
        "TOKEN_MODE = 'word'\n",
        "\n",
        "# Minimum document/corpus frequency below which a token will be discarded.\n",
        "MIN_DOCUMENT_FREQUENCY = 2\n",
        "\n",
        "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
        "    \"\"\"Vectorizes texts as n-gram vectors.\n",
        "\n",
        "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
        "\n",
        "    # Arguments\n",
        "        train_texts: list, training text strings.\n",
        "        train_labels: np.ndarray, training labels.\n",
        "        val_texts: list, validation text strings.\n",
        "\n",
        "    # Returns\n",
        "        x_train, x_val: vectorized training and validation texts\n",
        "    \"\"\"\n",
        "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
        "    kwargs = {\n",
        "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
        "            'dtype': 'float64',\n",
        "            'strip_accents': 'unicode',\n",
        "            'decode_error': 'replace',\n",
        "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
        "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
        "    }\n",
        "    vectorizer = TfidfVectorizer(**kwargs)\n",
        "\n",
        "    # Learn vocabulary from training texts and vectorize training texts.\n",
        "    x_train = vectorizer.fit_transform(train_texts)\n",
        "\n",
        "    # Vectorize validation texts.\n",
        "    x_val = vectorizer.transform(val_texts)\n",
        "\n",
        "    # Select top 'k' of the vectorized features.\n",
        "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
        "    selector.fit(x_train, train_labels)\n",
        "    x_train = selector.transform(x_train).astype('float32')\n",
        "    x_val = selector.transform(x_val).astype('float32')\n",
        "    return x_train, x_val\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNCTBjboRdhg",
        "outputId": "3171bd5d-a80c-478c-8791-c033bb41dff3"
      },
      "source": [
        "train_texts, test_texts = ngram_vectorize(train_texts,train_labels,test_texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:1817: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. float64 'dtype' will be converted to np.float64.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beTPg_llYhsl"
      },
      "source": [
        "###Build neural network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv5DUMLSRizE"
      },
      "source": [
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import Dropout\n",
        "\n",
        "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
        "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
        "\n",
        "    # Arguments\n",
        "        layers: int, number of `Dense` layers in the model.\n",
        "        units: int, output dimension of the layers.\n",
        "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
        "        input_shape: tuple, shape of input to the model.\n",
        "        num_classes: int, number of output classes.\n",
        "\n",
        "    # Returns\n",
        "        An MLP model instance.\n",
        "    \"\"\"\n",
        "    #op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
        "    if num_classes == 2:\n",
        "        op_activation = 'sigmoid'\n",
        "        op_units = 1\n",
        "    else:\n",
        "        op_activation = 'softmax'\n",
        "        op_units = num_classes\n",
        "        \n",
        "    model = models.Sequential()\n",
        "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
        "\n",
        "    for _ in range(layers-1):\n",
        "        model.add(Dense(units=units, activation='relu'))\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "        #tf.keras.layers.Lambda(tf.sparse.to_dense)(x)\n",
        "\n",
        "    model.add(Dense(units=op_units, activation=op_activation))\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OXtKzG8Rsd0"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "loss= 'sparse_categorical_crossentropy'\n",
        "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "num_classes = 5\n",
        "dropout_rate = 0.2\n",
        "batch_size =128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6isjLpz3NCv",
        "outputId": "f7236a16-3640-4db6-b353-45efd5e42ab3"
      },
      "source": [
        "train_texts.shape[1:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbCZGFUPR01P"
      },
      "source": [
        "model = mlp_model(layers=2,units=64,dropout_rate=0.2,input_shape = train_texts.shape[1:],num_classes=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yndpKKjZ2i_0",
        "outputId": "949514ba-312c-492d-8ef2-d62b135f2858"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout (Dropout)            (None, 20000)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                1280064   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 1,280,389\n",
            "Trainable params: 1,280,389\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsOzsB6hR10f"
      },
      "source": [
        "#model = models.Sequential()\n",
        "#model.add(Dense(64, activation='relu', input_dim=train_texts.shape[1]))\n",
        "#model.add(Dense(5, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekd1U9lPR8Je"
      },
      "source": [
        "model.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy',metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL-z0vvVR-y-"
      },
      "source": [
        "# Create callback for early stopping on validation loss. If the loss does\n",
        "    # not decrease in two consecutive tries, stop training.\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', patience=2) ]\n",
        "#, tf.keras.callbacks.LambdaCallback(on_batch_end=lambda batch,logs:print(logs))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtMT_UYJSCCy"
      },
      "source": [
        "def batch_generator(X, y, batch_size):\n",
        "    number_of_batches = X.shape[0]/batch_size\n",
        "    counter=0\n",
        "    shuffle_index = np.arange(np.shape(y)[0])\n",
        "    np.random.shuffle(shuffle_index)\n",
        "    X =  X[shuffle_index, :]\n",
        "    y =  y[shuffle_index]\n",
        "    while 1:\n",
        "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
        "        X_batch = X[index_batch,:].todense()\n",
        "        y_batch = y[index_batch]\n",
        "        counter += 1\n",
        "        yield(np.array(X_batch),y_batch)\n",
        "        if (counter >= number_of_batches):\n",
        "            np.random.shuffle(shuffle_index)\n",
        "            counter=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9c63vJYc9YR"
      },
      "source": [
        "epochs = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gYBxjQWSE3I",
        "outputId": "c9712e84-33e9-4218-c1f6-5bbfec69be9b"
      },
      "source": [
        "history = model.fit_generator(batch_generator(train_texts, train_labels, batch_size), \n",
        "                    epochs=epochs,steps_per_epoch = train_texts.shape[0]/batch_size, \n",
        "                    validation_data=batch_generator(test_texts, test_labels, batch_size*2),\n",
        "                    validation_steps=test_texts.shape[0]/batch_size*2,\n",
        "                    verbose=2,callbacks= callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "390/390 - 25s - loss: 1.3161 - acc: 0.4798 - val_loss: 1.1070 - val_acc: 0.5420\n",
            "Epoch 2/1000\n",
            "390/390 - 23s - loss: 0.9947 - acc: 0.5991 - val_loss: 1.0440 - val_acc: 0.5529\n",
            "Epoch 3/1000\n",
            "390/390 - 23s - loss: 0.8749 - acc: 0.6531 - val_loss: 1.0459 - val_acc: 0.5463\n",
            "Epoch 4/1000\n",
            "390/390 - 23s - loss: 0.7859 - acc: 0.6965 - val_loss: 1.0675 - val_acc: 0.5402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyBgFKqE9YZm",
        "outputId": "42cb917c-ae0a-4c65-8bc3-9cdf1a4b455a"
      },
      "source": [
        "test_texts.shape[0] // batch_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "156"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey8MQ_S8aqdC",
        "outputId": "37b4a00c-b534-409d-c258-72b099f343cd"
      },
      "source": [
        "epochs = 100\n",
        "history = model.fit_generator(batch_generator(train_texts, train_labels, batch_size), \n",
        "                    epochs=epochs,steps_per_epoch = train_texts.shape[0]/batch_size, \n",
        "                    validation_data=batch_generator(test_texts, test_labels, batch_size*2),\n",
        "                    validation_steps = test_texts.shape[0] // batch_size*2,\n",
        "                    callbacks= callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "11718/11718 [==============================] - 800s 68ms/step - loss: 0.9898 - acc: 0.5735 - val_loss: 0.9626 - val_acc: 0.5815\n",
            "Epoch 2/100\n",
            "11718/11718 [==============================] - 832s 71ms/step - loss: 0.9722 - acc: 0.5819 - val_loss: 0.9584 - val_acc: 0.5875\n",
            "Epoch 3/100\n",
            "11718/11718 [==============================] - 819s 70ms/step - loss: 0.9617 - acc: 0.5878 - val_loss: 0.9577 - val_acc: 0.5882\n",
            "Epoch 4/100\n",
            "11718/11718 [==============================] - 811s 69ms/step - loss: 0.9531 - acc: 0.5930 - val_loss: 0.9594 - val_acc: 0.5883\n",
            "Epoch 5/100\n",
            "11718/11718 [==============================] - 822s 70ms/step - loss: 0.9463 - acc: 0.5964 - val_loss: 0.9617 - val_acc: 0.5858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh9mbPGDZ2rk"
      },
      "source": [
        "### Prepare data for sequence model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVMnR5BbUhbo"
      },
      "source": [
        "from tensorflow.python.keras.preprocessing import sequence\n",
        "from tensorflow.python.keras.preprocessing import text\n",
        "\n",
        "# Vectorization parameters\n",
        "# Limit on the number of features. We use the top 20K features.\n",
        "TOP_K = 20000\n",
        "\n",
        "# Limit on the length of text sequences. Sequences longer than this\n",
        "# will be truncated.\n",
        "MAX_SEQUENCE_LENGTH = 500\n",
        "\n",
        "def sequence_vectorize(train_texts, val_texts):\n",
        "    \"\"\"Vectorizes texts as sequence vectors.\n",
        "\n",
        "    1 text = 1 sequence vector with fixed length.\n",
        "\n",
        "    # Arguments\n",
        "        train_texts: list, training text strings.\n",
        "        val_texts: list, validation text strings.\n",
        "\n",
        "    # Returns\n",
        "        x_train, x_val, word_index: vectorized training and validation\n",
        "            texts and word index dictionary.\n",
        "    \"\"\"\n",
        "    # Create vocabulary with training texts.\n",
        "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
        "    tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "    # Vectorize training and validation texts.\n",
        "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
        "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
        "\n",
        "    # Get max sequence length.\n",
        "    max_length = len(max(x_train, key=len))\n",
        "    if max_length > MAX_SEQUENCE_LENGTH:\n",
        "        max_length = MAX_SEQUENCE_LENGTH\n",
        "\n",
        "    # Fix sequence length to max value. Sequences shorter than the length are\n",
        "    # padded in the beginning and sequences longer are truncated\n",
        "    # at the beginning.\n",
        "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
        "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
        "    return x_train, x_val, tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT40nepcaBjq"
      },
      "source": [
        "x_train , x_val , tokenized_word_index = sequence_vectorize(train_texts,test_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1KfqicQWK_J"
      },
      "source": [
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import initializers\n",
        "from tensorflow.python.keras import regularizers\n",
        "\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import Dropout\n",
        "from tensorflow.python.keras.layers import Embedding\n",
        "from tensorflow.python.keras.layers import SeparableConv1D\n",
        "from tensorflow.python.keras.layers import MaxPooling1D\n",
        "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
        "\n",
        "def sepcnn_model(blocks,\n",
        "                 filters,\n",
        "                 kernel_size,\n",
        "                 embedding_dim,\n",
        "                 dropout_rate,\n",
        "                 pool_size,\n",
        "                 input_shape,\n",
        "                 num_classes,\n",
        "                 num_features,\n",
        "                 use_pretrained_embedding=False,\n",
        "                 is_embedding_trainable=False,\n",
        "                 embedding_matrix=None):\n",
        "    \"\"\"Creates an instance of a separable CNN model.\n",
        "\n",
        "    # Arguments\n",
        "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
        "        filters: int, output dimension of the layers.\n",
        "        kernel_size: int, length of the convolution window.\n",
        "        embedding_dim: int, dimension of the embedding vectors.\n",
        "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
        "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
        "        input_shape: tuple, shape of input to the model.\n",
        "        num_classes: int, number of output classes.\n",
        "        num_features: int, number of words (embedding input dimension).\n",
        "        use_pretrained_embedding: bool, true if pre-trained embedding is on.\n",
        "        is_embedding_trainable: bool, true if embedding layer is trainable.\n",
        "        embedding_matrix: dict, dictionary with embedding coefficients.\n",
        "\n",
        "    # Returns\n",
        "        A sepCNN model instance.\n",
        "    \"\"\"\n",
        "    #op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
        "    if num_classes == 2:\n",
        "        op_activation = 'sigmoid'\n",
        "        op_units = 1\n",
        "    else:\n",
        "        op_activation = 'softmax'\n",
        "        op_units = num_classes\n",
        "        \n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
        "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
        "    if use_pretrained_embedding:\n",
        "        model.add(Embedding(input_dim=num_features,\n",
        "                            output_dim=embedding_dim,\n",
        "                            input_length=input_shape[0],\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=is_embedding_trainable))\n",
        "    else:\n",
        "        model.add(Embedding(input_dim=num_features,\n",
        "                            output_dim=embedding_dim,\n",
        "                            input_length=input_shape[0]))\n",
        "\n",
        "    for _ in range(blocks-1):\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "        model.add(SeparableConv1D(filters=filters,\n",
        "                                  kernel_size=kernel_size,\n",
        "                                  activation='relu',\n",
        "                                  bias_initializer='random_uniform',\n",
        "                                  depthwise_initializer='random_uniform',\n",
        "                                  padding='same'))\n",
        "        model.add(SeparableConv1D(filters=filters,\n",
        "                                  kernel_size=kernel_size,\n",
        "                                  activation='relu',\n",
        "                                  bias_initializer='random_uniform',\n",
        "                                  depthwise_initializer='random_uniform',\n",
        "                                  padding='same'))\n",
        "        model.add(MaxPooling1D(pool_size=pool_size))\n",
        "\n",
        "    model.add(SeparableConv1D(filters=filters * 2,\n",
        "                              kernel_size=kernel_size,\n",
        "                              activation='relu',\n",
        "                              bias_initializer='random_uniform',\n",
        "                              depthwise_initializer='random_uniform',\n",
        "                              padding='same'))\n",
        "    model.add(SeparableConv1D(filters=filters * 2,\n",
        "                              kernel_size=kernel_size,\n",
        "                              activation='relu',\n",
        "                              bias_initializer='random_uniform',\n",
        "                              depthwise_initializer='random_uniform',\n",
        "                              padding='same'))\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    model.add(Dropout(rate=dropout_rate))\n",
        "    model.add(Dense(op_units, activation=op_activation))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "YV7bAuzHXEW2",
        "outputId": "35240aff-af76-4204-e57b-c1d1d02b8389"
      },
      "source": [
        "learning_rate_set=1e-4\n",
        "epochs=5\n",
        "batch_size=256\n",
        "blocks=2\n",
        "filters=64\n",
        "dropout_rate=0.2\n",
        "embedding_dim=200\n",
        "kernel_size=3\n",
        "pool_size=3\n",
        "num_classes = 5\n",
        "# Limit on the number of features. We use the top 20K features.\n",
        "TOP_K = 20000\n",
        "num_features = min(len(tokenized_word_index) + 1, TOP_K)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9772b5f76728>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Limit on the number of features. We use the top 20K features.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mTOP_K\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_word_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTOP_K\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenized_word_index' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv1QL6COWvvk"
      },
      "source": [
        "# Create model instance.\n",
        "model = sepcnn_model(blocks=blocks,\n",
        "                                     filters=filters,\n",
        "                                     kernel_size=kernel_size,\n",
        "                                     embedding_dim=embedding_dim,\n",
        "                                     dropout_rate=dropout_rate,\n",
        "                                     pool_size=pool_size,\n",
        "                                     input_shape=x_train.shape[1:],\n",
        "                                     num_classes=num_classes,\n",
        "                                     num_features=num_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9bUSOU4ci1B",
        "outputId": "64e9ecbd-c462-459b-e547-403980fde55a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 241, 200)          4000000   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 241, 200)          0         \n",
            "_________________________________________________________________\n",
            "separable_conv1d (SeparableC (None, 241, 64)           13464     \n",
            "_________________________________________________________________\n",
            "separable_conv1d_1 (Separabl (None, 241, 64)           4352      \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 80, 64)            0         \n",
            "_________________________________________________________________\n",
            "separable_conv1d_2 (Separabl (None, 80, 128)           8512      \n",
            "_________________________________________________________________\n",
            "separable_conv1d_3 (Separabl (None, 80, 128)           16896     \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 4,043,869\n",
            "Trainable params: 4,043,869\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3QX9S9EUjcx"
      },
      "source": [
        "###Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfDEf0JmlryG"
      },
      "source": [
        "def _data_generator(x, y, num_features, batch_size):\n",
        "    \"\"\"Generates batches of vectorized texts for training/validation.\n",
        "    # Arguments\n",
        "        x: np.matrix, feature matrix.\n",
        "        y: np.ndarray, labels.\n",
        "        num_features: int, number of features.\n",
        "        batch_size: int, number of samples per batch.\n",
        "    # Returns\n",
        "        Yields feature and label data in batches.\n",
        "    \"\"\"\n",
        "    num_samples = x.shape[0]\n",
        "    num_batches = num_samples // batch_size\n",
        "    if num_samples % batch_size:\n",
        "        num_batches += 1\n",
        "\n",
        "    while 1:\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * batch_size\n",
        "            end_idx = (i + 1) * batch_size\n",
        "            if end_idx > num_samples:\n",
        "                end_idx = num_samples\n",
        "            x_batch = x[start_idx:end_idx]\n",
        "            y_batch = y[start_idx:end_idx]\n",
        "            yield x_batch, y_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ztA5CInJSvm",
        "outputId": "0d87482c-0a46-47ab-f156-8210ae109d67"
      },
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/text_classification_training/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "os.listdir(checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cp-0001.ckpt.index',\n",
              " 'cp-0001.ckpt.data-00000-of-00001',\n",
              " 'cp-0002.ckpt.index',\n",
              " 'cp-0002.ckpt.data-00000-of-00001',\n",
              " 'cp-0003.ckpt.index',\n",
              " 'cp-0003.ckpt.data-00000-of-00001',\n",
              " 'cp-0004.ckpt.index',\n",
              " 'cp-0004.ckpt.data-00000-of-00001',\n",
              " 'cp-0005.ckpt.index',\n",
              " 'checkpoint',\n",
              " 'cp-0005.ckpt.data-00000-of-00001',\n",
              " 'my_model']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "K4kXKCikJe_K",
        "outputId": "60943594-9958-451a-f76e-8dd9241aebae"
      },
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "latest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/text_classification_training/cp-0005.ckpt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2DUz71tJi4B",
        "outputId": "314c7faf-d7c8-4dad-8439-93577cc52293"
      },
      "source": [
        "# Load the previously saved weights\n",
        "model.load_weights(latest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f581f740c90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzTx7NjJdRrh"
      },
      "source": [
        "# Compile model with learning parameters.\n",
        "if num_classes == 2:\n",
        "    loss = 'binary_crossentropy'\n",
        "else:\n",
        "    loss = 'sparse_categorical_crossentropy'\n",
        " \n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_set)\n",
        " \n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_qp7PIuWoNo",
        "outputId": "361561a2-0201-4b9a-bac9-675ab89c262d"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.sequential.Sequential at 0x7f5827875150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nrtmHd8WeLn"
      },
      "source": [
        "###Keras Tuner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkp-KmnSWeLo"
      },
      "source": [
        "!pip install keras-tuner --upgrade\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j42vrkgWeLp"
      },
      "source": [
        "import keras_tuner as kt\n",
        "tuner = kt.Hyperband(\n",
        "    model,\n",
        "    objective='val_loss',\n",
        "    max_epochs=5)\n",
        "\n",
        "tuner.search(x_train, train_labels, epochs=5, validation_data=(x_val, test_labels))\n",
        "best_model = tuner.get_best_models()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU83WCZ5WU7t",
        "outputId": "cbf79494-90d4-46b1-f13f-89537b2826ba"
      },
      "source": [
        "# Create callback for early stopping on validation loss. If the loss does\n",
        "# not decrease in two consecutive tries, stop training.\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=2)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create a callback that saves the model's weights every 1 epoch\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    verbose=1, \n",
        "    save_weights_only=True,\n",
        "    save_freq=batch_size)\n",
        " \n",
        "# Create training and validation generators.\n",
        "training_generator = _data_generator(\n",
        "    x_train, train_labels, num_features, batch_size)\n",
        "validation_generator = _data_generator(\n",
        "    x_val, test_labels, num_features, batch_size)\n",
        " \n",
        "# Get number of training steps. This indicated the number of steps it takes\n",
        "# to cover all samples in one epoch.\n",
        "steps_per_epoch = x_train.shape[0] // batch_size\n",
        "if x_train.shape[0] % batch_size:\n",
        "    steps_per_epoch += 1\n",
        " \n",
        "# Get number of validation steps.\n",
        "validation_steps = x_val.shape[0] // batch_size\n",
        "if x_val.shape[0] % batch_size:\n",
        "    validation_steps += 1\n",
        " \n",
        "# Train and validate model.\n",
        "history = model.fit_generator(\n",
        "        generator=training_generator,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=validation_steps,\n",
        "        callbacks=cp_callback,\n",
        "        epochs=epochs\n",
        "        #,verbose=2\n",
        "        )  # Logs once per epoch.\n",
        " \n",
        "# Print results.\n",
        "history = history.history\n",
        "print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
        "        acc=history['val_acc'][-1], loss=history['val_loss'][-1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpdVvchkX5MX",
        "outputId": "47fab4cf-7f6b-4bc3-cc83-213106a35fe6"
      },
      "source": [
        "import sys\n",
        "sys.getsizeof(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yri-T-DPn7oQ",
        "outputId": "9bde773f-ef16-44b1-e05b-5853b2531d26"
      },
      "source": [
        "model.save('/content/drive/MyDrive/text_classification_training/my_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/text_classification_training/my_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPwu0tT0ewxq"
      },
      "source": [
        "##bert model application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auzGg3gMgB7B"
      },
      "source": [
        "##steup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c-E6yuRgESt",
        "outputId": "6c87de84-619f-4404-a5e0-2827640441c8"
      },
      "source": [
        "# A dependency of the preprocessing for BERT inputs\n",
        "!pip install -q -U tensorflow-text\n",
        "!pip install -q tf-models-official\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.3MB 8.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6MB 7.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 45.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 13.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 43.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 10.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 49.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 38.2MB 68kB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 9.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 45.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 686kB 52.2MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL5ZJLmLewCN",
        "outputId": "89c49a1e-f4c4-4e76-e3ae-2b51068b9e5e"
      },
      "source": [
        "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MFu63MyfB2B"
      },
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4-B9dzCfuTQ",
        "outputId": "cda22560-8bba-45b0-c615-d837b864517c"
      },
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "text_test = ['this is such an amazing movie!']\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "\n",
        "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
        "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
        "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
        "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keys       : ['input_type_ids', 'input_word_ids', 'input_mask']\n",
            "Shape      : (1, 128)\n",
            "Word Ids   : [ 101 2023 2003 2107 2019 6429 3185  999  102    0    0    0]\n",
            "Input Mask : [1 1 1 1 1 1 1 1 1 0 0 0]\n",
            "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olPErs8mggOU",
        "outputId": "4c6df7ea-1b3e-464b-c410-b098b4fcbdc6"
      },
      "source": [
        "classifier_model = build_classifier_model()\n",
        "bert_raw_result = classifier_model(tf.constant(text_test))\n",
        "print(tf.sigmoid(bert_raw_result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.15927008]], shape=(1, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "9uQPohqtgcDo",
        "outputId": "b41c7373-4f67-4baf-be9d-9d167e212cd7"
      },
      "source": [
        "tf.keras.utils.plot_model(classifier_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeR5reXygphN"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "u4qHfMGdgwQc",
        "outputId": "f3817731-6953-4875-f05a-d5a0bc9bd2cf"
      },
      "source": [
        "epochs = 5\n",
        "#steps_per_epoch = tf.data.experimental.cardinality(x_train).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-2e3d56732aa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#steps_per_epoch = tf.data.experimental.cardinality(x_train).numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnum_train_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnum_warmup_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_train_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'steps_per_epoch' is not defined"
          ]
        }
      ]
    }
  ]
}